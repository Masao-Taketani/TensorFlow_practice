# BERT

## References
Paper: [BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding](https://arxiv.org/abs/1810.04805)<br>
Codes: https://github.com/google-research/bert<br>
