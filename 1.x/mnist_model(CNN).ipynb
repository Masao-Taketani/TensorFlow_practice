{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# get the dataset\n",
    "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_X, train_Y, test_X, test_Y = mnist_data.train.images, \\\n",
    "mnist_data.train.labels, mnist_data.test.images, mnist_data.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyper params\n",
    "lr = 0.01\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "num_iters_per_epoch = train_X.shape[0] // batch_size\n",
    "height = width = int(np.sqrt(train_X.shape[1]))\n",
    "num_classes = train_Y.shape[1]\n",
    "dropout_rate = 0.5\n",
    "fil_h = 5\n",
    "fil_w = 5\n",
    "inp_dep = 1\n",
    "hid1_dep = 64\n",
    "hid2_dep = hid1_dep * 2\n",
    "flatten_units = (height//4)*(width//4)*hid2_dep\n",
    "dense_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholders and variables\n",
    "X = tf.placeholder(tf.float32, [None, height*width])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "weights = {\n",
    "    \"conv_w1\": tf.Variable(tf.random_normal([fil_h, fil_w, inp_dep, hid1_dep])),\n",
    "    \"conv_w2\": tf.Variable(tf.random_normal([fil_h, fil_w, hid1_dep, hid2_dep])),\n",
    "    \"dense_w1\": tf.Variable(tf.random_normal([flatten_units, dense_units])),\n",
    "    \"dense_w2\": tf.Variable(tf.random_normal([dense_units, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"conv_b1\": tf.Variable(tf.ones([hid1_dep])),\n",
    "    \"conv_b2\": tf.Variable(tf.ones([hid2_dep])),\n",
    "    \"dense_b1\": tf.Variable(tf.ones([dense_units])),\n",
    "    \"dense_b2\": tf.Variable(tf.ones([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions needed to make the CNN network\n",
    "def conv2d(x, w, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding=\"SAME\")\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpooling2d(x, strides=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, strides, strides, 1], strides=[1, strides, strides, 1], padding=\"SAME\")\n",
    "\n",
    "def mnist_conv_net(x, weights, biases, dropout_rate):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    x = conv2d(x, weights[\"conv_w1\"], biases[\"conv_b1\"])\n",
    "    x = maxpooling2d(x, 2)\n",
    "    x = conv2d(x, weights[\"conv_w2\"], biases[\"conv_b2\"])\n",
    "    x = maxpooling2d(x, 2)\n",
    "    x = tf.reshape(x, [-1, flatten_units])\n",
    "    x = tf.add(tf.matmul(x, weights[\"dense_w1\"]), biases[\"dense_b1\"])\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.dropout(x, dropout_rate)\n",
    "    out = tf.add(tf.matmul(x, weights[\"dense_w2\"]), biases[\"dense_b2\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 acc:295.062 loss:29720.294 test_acc:nan test_loss:nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-3fba7ef2b0d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                           \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                           keep_prob: dropout_rate})\n\u001b[0;32m---> 27\u001b[0;31m             tmp_loss += sess.run(loss, feed_dict={X: train_X[rand_indexes][it*batch_size:(it+1)*batch_size],\\\n\u001b[0m\u001b[1;32m     28\u001b[0m                                                  \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                  keep_prob: dropout_rate}) / batch_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_log = []\n",
    "test_loss_log = []\n",
    "acc_log = []\n",
    "test_acc_log = []\n",
    "\n",
    "pred = mnist_conv_net(X, weights, biases, keep_prob)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=Y))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1)), tf.float32))\n",
    "\n",
    "# to measure total training time\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(epochs):\n",
    "        # shuffle dataset for every epoch\n",
    "        rand_indexes = np.random.permutation(train_X.shape[0])\n",
    "        tmp_loss = 0.0\n",
    "        tmp_test_loss = 0.0\n",
    "        tmp_acc = 0.0\n",
    "        tmp_test_acc = 0.0\n",
    "        \n",
    "        for it in range(num_iters_per_epoch):\n",
    "            sess.run(train_op, feed_dict={X: train_X[rand_indexes][it*batch_size:(it+1)*batch_size],\\\n",
    "                                          Y: train_Y[rand_indexes][it*batch_size:(it+1)*batch_size],\\\n",
    "                                          keep_prob: dropout_rate})\n",
    "            tmp_loss += sess.run(loss, feed_dict={X: train_X[rand_indexes][it*batch_size:(it+1)*batch_size],\\\n",
    "                                                 Y: train_Y[rand_indexes][it*batch_size:(it+1)*batch_size],\\\n",
    "                                                 keep_prob: dropout_rate}) / batch_size\n",
    "            tmp_test_loss += sess.run(loss, feed_dict={X: test_X[it*batch_size:(it+1)*batch_size],\\\n",
    "                                                       Y: test_Y[it*batch_size:(it+1)*batch_size],\\\n",
    "                                                       keep_prob: 1.0}) / batch_size\n",
    "            tmp_acc += sess.run(acc, feed_dict={X: train_X[rand_indexes][it*batch_size:(it+1)*batch_size],\\\n",
    "                                                Y: train_Y[rand_indexes][it*batch_size:(it+1)*batch_size],\\\n",
    "                                                keep_prob: dropout_rate})\n",
    "            tmp_test_acc += sess.run(acc, feed_dict={X: test_X[it*batch_size:(it+1)*batch_size],\\\n",
    "                                                     Y: test_Y[it*batch_size:(it+1)*batch_size],\\\n",
    "                                                     keep_prob: 1.0})\n",
    "        \n",
    "        loss_log.append(tmp_loss / num_iters_per_epoch)\n",
    "        test_loss_log.append(tmp_test_loss / num_iters_per_epoch)\n",
    "        acc_log.append(tmp_acc / num_iters_per_epoch)\n",
    "        test_acc_log.append(tmp_test_acc / num_iters_per_epoch)\n",
    "        \n",
    "        print(\"epoch:{} acc:{:.3f} loss:{:.3f} test_acc:{:.3f} test_loss:{:.3f}\"\\\n",
    "              .format(ep+1, tmp_acc, tmp_loss, tmp_test_acc, tmp_test_loss))\n",
    "        \n",
    "end_time = time.time()\n",
    "print(\"Total Processing Time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(784)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
