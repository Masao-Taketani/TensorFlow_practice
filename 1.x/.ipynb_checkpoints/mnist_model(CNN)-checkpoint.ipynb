{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# get the dataset\n",
    "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_X, train_Y, test_X, test_Y = mnist_data.train.images, \\\n",
    "mnist_data.train.labels, mnist_data.test.images, mnist_data.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyper params\n",
    "lr = 0.0001\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "num_iters_per_epoch = train_X.shape[0] // batch_size\n",
    "height = width = int(np.sqrt(train_X.shape[1]))\n",
    "channel = 1\n",
    "num_classes = train_Y.shape[1]\n",
    "dropout_rate = 0.75\n",
    "fil_h = 5\n",
    "fil_w = 5\n",
    "inp_dep = 1\n",
    "hid1_dep = 64\n",
    "hid2_dep = hid1_dep * 2\n",
    "flatten_units = (height//4)*(width//4)*hid2_dep\n",
    "dense_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholders and variables\n",
    "X = tf.placeholder(tf.float32, [None, height*width])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "weights = {\n",
    "    \"conv_w1\": tf.Variable(tf.random_normal([fil_h, fil_w, inp_dep, hid1_dep], stddev=0.01)),\n",
    "    \"conv_w2\": tf.Variable(tf.random_normal([fil_h, fil_w, hid1_dep, hid2_dep], stddev=0.01)),\n",
    "    \"dense_w1\": tf.Variable(tf.random_normal([flatten_units, dense_units])),\n",
    "    \"dense_w2\": tf.Variable(tf.random_normal([dense_units, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"conv_b1\": tf.Variable(tf.zeros([hid1_dep])),\n",
    "    \"conv_b2\": tf.Variable(tf.zeros([hid2_dep])),\n",
    "    \"dense_b1\": tf.Variable(tf.zeros([dense_units])),\n",
    "    \"dense_b2\": tf.Variable(tf.zeros([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions needed to make the CNN network\n",
    "def conv2d(x, w, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding=\"SAME\")\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpooling2d(x, strides=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, strides, strides, 1], strides=[1, strides, strides, 1], padding=\"SAME\")\n",
    "\n",
    "def mnist_conv_net(x, weights, biases, dropout_rate):\n",
    "    x = tf.reshape(x, shape=[-1, height, width, channel])\n",
    "    x = conv2d(x, weights[\"conv_w1\"], biases[\"conv_b1\"])\n",
    "    x = maxpooling2d(x, 2)\n",
    "    x = conv2d(x, weights[\"conv_w2\"], biases[\"conv_b2\"])\n",
    "    x = maxpooling2d(x, 2)\n",
    "    x = tf.reshape(x, [-1, flatten_units])\n",
    "    x = tf.add(tf.matmul(x, weights[\"dense_w1\"]), biases[\"dense_b1\"])\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.dropout(x, dropout_rate)\n",
    "    out = tf.add(tf.matmul(x, weights[\"dense_w2\"]), biases[\"dense_b2\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 acc:0.887 loss:0.547 test_acc:0.972 test_loss:0.085\n",
      "epoch:2 acc:0.962 loss:0.123 test_acc:0.981 test_loss:0.059\n",
      "epoch:3 acc:0.974 loss:0.085 test_acc:0.986 test_loss:0.046\n",
      "epoch:4 acc:0.979 loss:0.068 test_acc:0.986 test_loss:0.039\n",
      "epoch:5 acc:0.983 loss:0.053 test_acc:0.988 test_loss:0.036\n",
      "epoch:6 acc:0.985 loss:0.048 test_acc:0.989 test_loss:0.038\n",
      "epoch:7 acc:0.988 loss:0.038 test_acc:0.989 test_loss:0.033\n",
      "epoch:8 acc:0.989 loss:0.035 test_acc:0.990 test_loss:0.036\n",
      "epoch:9 acc:0.990 loss:0.030 test_acc:0.989 test_loss:0.036\n",
      "epoch:10 acc:0.992 loss:0.024 test_acc:0.991 test_loss:0.029\n",
      "epoch:11 acc:0.992 loss:0.025 test_acc:0.990 test_loss:0.033\n",
      "epoch:12 acc:0.993 loss:0.020 test_acc:0.989 test_loss:0.041\n",
      "epoch:13 acc:0.994 loss:0.019 test_acc:0.991 test_loss:0.032\n",
      "epoch:14 acc:0.995 loss:0.016 test_acc:0.990 test_loss:0.040\n",
      "epoch:15 acc:0.995 loss:0.017 test_acc:0.991 test_loss:0.037\n",
      "epoch:16 acc:0.995 loss:0.014 test_acc:0.991 test_loss:0.036\n",
      "epoch:17 acc:0.996 loss:0.013 test_acc:0.990 test_loss:0.042\n",
      "epoch:18 acc:0.996 loss:0.011 test_acc:0.991 test_loss:0.044\n",
      "epoch:19 acc:0.997 loss:0.010 test_acc:0.990 test_loss:0.045\n",
      "epoch:20 acc:0.996 loss:0.011 test_acc:0.992 test_loss:0.036\n",
      "epoch:21 acc:0.997 loss:0.012 test_acc:0.991 test_loss:0.039\n",
      "epoch:22 acc:0.997 loss:0.009 test_acc:0.992 test_loss:0.041\n",
      "epoch:23 acc:0.997 loss:0.010 test_acc:0.990 test_loss:0.042\n",
      "epoch:24 acc:0.997 loss:0.009 test_acc:0.991 test_loss:0.044\n",
      "epoch:25 acc:0.998 loss:0.008 test_acc:0.992 test_loss:0.044\n",
      "epoch:26 acc:0.997 loss:0.008 test_acc:0.992 test_loss:0.043\n",
      "epoch:27 acc:0.997 loss:0.009 test_acc:0.990 test_loss:0.052\n",
      "epoch:28 acc:0.997 loss:0.009 test_acc:0.991 test_loss:0.041\n",
      "epoch:29 acc:0.998 loss:0.006 test_acc:0.992 test_loss:0.039\n",
      "epoch:30 acc:0.998 loss:0.008 test_acc:0.992 test_loss:0.045\n",
      "Total Processing Time: 1075.9158651828766\n"
     ]
    }
   ],
   "source": [
    "loss_log = []\n",
    "test_loss_log = []\n",
    "acc_log = []\n",
    "test_acc_log = []\n",
    "\n",
    "pred = mnist_conv_net(X, weights, biases, keep_prob)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=pred))\n",
    "opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = opt.minimize(loss)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1)), tf.float32))\n",
    "\n",
    "# to measure total training time\n",
    "start_time = time.time()\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        # shuffle dataset for every epoch\n",
    "        rand_indexes = np.random.permutation(train_X.shape[0])\n",
    "        tmp_acc = 0.0\n",
    "        tmp_loss = 0.0\n",
    "        tmp_test_acc = 0.0\n",
    "        tmp_test_loss = 0.0\n",
    "        \n",
    "        for it in range(num_iters_per_epoch):\n",
    "            _, it_acc, it_loss = sess.run([train_op, acc, loss], feed_dict={X: train_X[rand_indexes][it*batch_size:(it+1)*batch_size],\n",
    "                                          Y: train_Y[rand_indexes][it*batch_size:(it+1)*batch_size],\n",
    "                                          keep_prob: dropout_rate})\n",
    "\n",
    "            tmp_acc += it_acc / num_iters_per_epoch\n",
    "            tmp_loss += it_loss / num_iters_per_epoch\n",
    "        \n",
    "        tmp_test_acc, tmp_test_loss = sess.run([acc, loss], feed_dict={X: test_X, Y: test_Y, keep_prob: 1.0})\n",
    "        \n",
    "        acc_log.append(tmp_acc)\n",
    "        loss_log.append(tmp_loss)\n",
    "        test_acc_log.append(tmp_test_acc)\n",
    "        test_loss_log.append(tmp_test_loss)\n",
    "        \n",
    "        print(\"epoch:{} acc:{:.3f} loss:{:.3f} test_acc:{:.3f} test_loss:{:.3f}\"\\\n",
    "              .format(ep+1, acc_log[-1], loss_log[-1], test_acc_log[-1], test_loss_log[-1]))\n",
    "        \n",
    "end_time = time.time()\n",
    "print(\"Total Processing Time:\", end_time - start_time)\n",
    "\n",
    "# reset the default graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
